<!DOCTYPE html>
<html>

<head>
    <title>VENUE@ECCV 2024</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta name="description" content="Website for ECCV 2024 VENUE Tutorial">
    <meta name="keywords" content="" />
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>

    <!-- CSS  --> . 
    <link rel="stylesheet" href="./css/skel.css">
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="./css/style-wide.css">
    <link rel="stylesheet" href="./css/style-noscript.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">
    <!--[if lte IE 8]><link rel="stylesheet" href="./css/ie/v8.css" /><![endif]-->
    <!--[if lte IE 9]><link rel="stylesheet" href="./css/ie/v9.css" /><![endif]-->

    <!--[if lte IE 8]><script src="./css/ie/html5shiv.js"></script><![endif]-->
    <script src="./js/jquery.min.js"></script>
    <script src="./js/jquery.dropotron.min.js"></script>
    <script src="./js/jquery.scrolly.min.js"></script>
    <script src="./js/jquery.scrollgress.min.js"></script>
    <script src="./js/skel.min.js"></script>
    <script src="./js/skel-layers.min.js"></script>
    <script src="./js/init.js"></script>

    <meta name="viewport" content="width=device-width">

    <link rel="canonical" href="https://venue-tutorial.github.io/">
</head>

<style type="text/css">

  .schedule table {
    -webkit-border-radius: 5px;
    -moz-border-radius: 5px;
    border-radius: 5px;
    width: 100%;
    border-collapse: separate;
  }

  .schedule tr:nth-child(odd) td {
    background-color: #f9f9f9;
  }

  .schedule tr:nth-child(even) td {
    background-color: #e5e5e5;
  }

  .schedule tr td {
    padding: 1.5625em 1em;
    border: 1px solid #dcdcdc;
  }

</style>

<body class="index">
    <header id="header">
        <h1 id="logo"><a href="/">VENUE</a></h1>
        <nav id="nav">
            <ul>
                <li><a href="./index.html"><span class="label">Home</span></a></li>
            </ul>
        </nav>
    </header>

    <!-- Banner -->		
    <section id="banner" style="padding: 0%;" >
        <div class="video_container">
            <img style="object-fit: cover; position: absolute; top: 0; left: 0; width: 100%; height:100%;" src="./images/milan.png">
            <div class="inner">
                <header>
                    <h2>VENUE ECCV2024</h2>
                    <h3>Recent Advances in Video Content Understanding and Generation
                    </h3>
                </header>
                <p>
                Full-day Tutorial
                <br />
                Date: Sep 30th, AM&PM
                <br />
                Location: Amber 3, MiCo Milano, Milan, Italy
                </p>
            </div>
        </div>
    </section>

<div class="page-content">
<!-- Main -->
<article id="main">
  <!-- One -->
  <section class="wrapper style3 container">
    <!-- Content -->
    <div class="content">
      <section>
        <header>
          <h2 style="text-transform: none;">Recent Advances in Video Content Understanding and Generation</h2>
        </header>
        <p>
Over the last decade, tremendous interests have been attracted to this field, and great success has been achieved for various video-centric tasks (e.g., action recognition, motion capture and understanding, video understanding, temporal localization, and video generation etc.) based on conventional short videos. In recent years, with the explosion of videos and various application demands (e.g., video editing, AR/VR, human-robot interaction etc.), significantly more efforts are required to enable an intelligent system to understand and generate video content under different scenarios within multimodal, long-term and fine-grained inputs. Moreover, with the development of recent large language models (LLMs) and large multimodal models (LMMs), there are growing new trends and challenges to be discussed and addressed.
The goal of this tutorial is to foster interdisciplinary communication among researchers so that more attention from the broader community can be drawn to this field. This tutorial will discuss current progress and future directions, and new ideas and discoveries in related fields are expected to emerge. The tentative topics include but are not limited to:
        </p>
        <p>
            <ul>
              <li><strong>Video Content Understanding:</strong> human pose/mesh recovery from video; action recognition, localization, and segmentation; video captioning and video question answering with LLMs or LMMs; </li>
              <li><strong>Video Content Generation:</strong> basic models for video generation (diffusion model, GAN model, etc.); controllable video generation with multimodal inputs; 4D video creation;</li>
              <li><strong>Foundations and Beyond:</strong> large language models/large multimodal models for video representation learning, unified modelling of video understanding and generation, long-term video modeling, dataset, and evaluation. </li>
            </ul>
        </p>
      </section>
    </div>
  </section>

<section class="wrapper style3 container special" id="schedule">

    <header class="major">
      <h2><strong>Schedule</strong></h2>
  </header>

      <table class="schedule">
        <tbody>

          <col width="33%">
          <col width="33%">
          <col width="33%">

          <tr>
            <td><h4><strong>Time</strong></h4></td>
            <td><h4><strong>Speaker</strong></h4></td>
            <td><h4><strong>Content</strong></h4></td>
          </tr>

          <tr>
            <td><b>09:00 am - 09:45 am</b></td>
            <td><b>Mike Z. Shou</b></td>
            <td><b>Invited Talk1</b></td>
          </tr>

          <tr>
            <td><b>09:45 am - 10:30 am</b></td>
            <td><b>Angjoo Kanazawa</b></td>
            <td><b>Invited Talk2</b></td>
          </tr>

          <tr>
            <td><b>11:00 am - 11:45 am</b></td>
            <td><b>Chuang Gan</b></td>
            <td><b>Invited Talk3</b></td>
          </tr>

           <tr>
            <td><b>11:45 am - 12:30 am</b></td>
            <td><b>Hao Zhao</b></td>
            <td><b>Invited Talk4</b></td>
          </tr>

           <tr>
            <td><b>14:00 pm - 14:45 pm</b></td>
            <td><b>Cordelia Schmid</b></td>
            <td><b>Invited Talk5</b></td>
          </tr>

           <tr>
            <td><b>14:45 pm - 15:30 pm</b></td>
            <td><b>Yingqing He</b></td>
            <td><b>Invited Talk6</b></td>
          </tr>

           <tr>
            <td><b>16:00 pm - 16:45 pm</b></td>
            <td><b>Kashyap Chitta</b></td>
            <td><b>Invited Talk7</b></td>
          </tr>

           <tr>
            <td><b>16:45 pm - 17:30 pm</b></td>
            <td><b>Sherry Yang</b></td>
            <td><b>Invited Talk8</b></td>
          </tr>

      </tbody>

    </table>
</section>

<section class="wrapper style3 container special" id="talk-details">
    <header class="major">
      <h2><strong>Talk Details</strong></h2>
    </header>

    <div class="content">
        <h3>Mike Z. Shou</h3>
        <p><strong>Talk Title:</strong> Show-o: One Single Transformer to Unify Multimodal Understanding and Generation </p>
        <p><strong>Abstract:</strong> Exciting models have been developed in multimodal video understanding and generation, such as video LLM and video diffusion model. One emerging pathway to the ultimate intelligence is to create one single foundation model that can do both understanding and generation. After all, humans only use one brain to do both tasks. Towards such unification, recent attempts employ a base language model for multimodal understanding but require an additional pre-trained diffusion model for visual generation, which still remain as two separate components. In this work, we present Show-o, one single transformer that handles both multimodal understanding and generation. Unlike fully autoregressive models, Show-o is the first to unify autoregressive and discrete diffusion modeling, flexibly supporting a wide range of vision-language tasks including visual question-answering, text-to-image generation, text-guided inpainting/extrapolation, and mixed-modality generation of any input/output format, all within one single 1.3B transformer. Across various benchmarks, Show-o demonstrates comparable or superior performance, shedding light for building the next-generation video foundation model.</p>
        <p><strong>Bio:</strong> Mike Shou is a tenure-track Assistant Professor at National University of Singapore. He was a Research Scientist at Facebook AI in the Bay Area. He obtained his Ph.D. degree at Columbia University, working with Prof. Shih-Fu Chang. He received the Best Paper Finalist at CVPR 2022, Best Student Paper Nomination at CVPR 2017, PREMIA Best Paper Award 2023, EgoVis Distinguished Paper Award 2022/23. His team won 1st place in the international challenges including ActivityNet 2017, EPIC-Kitchens 2022, Ego4D 2022 & 2023. He is a Singapore Technologies Engineering Distinguished Professor and a Fellow of National Research Foundation Singapore. He is on the Forbes 30 Under 30 Asia list. </p>

        <h3>Hao Zhao</h3>
        <p><strong>Talk Title:</strong> Video Simulation and Holistic Understanding for Autonomous Driving: Systems and Backbones </p>
        <p><strong>Bio:</strong> Zhao Hao is an Assistant Professor at the Institute for AI Industry Research (AIR) at Tsinghua University. He received his Bachelor's and Ph.D. degrees from the Department of Electronic Engineering at Tsinghua University. He has worked as a research scientist at Intel Labs China and conducted postdoctoral research at Peking University. He has published over 50 research papers in academic conferences such as CVPR/NeurIPS/SIGGRAPH and journals such as T-PAMI/IJCV. He has won multiple championships in 3D scene understanding algorithm challenges and led the development of the world's first open-source modular realistic autonomous driving simulator, MARS, which won the Best Paper Runner-up award at CICAI 2023. His neural rendering method, SlimmeRF, which allows adjustable precision and speed during the rendering phase, won the Best Paper award at 3DV 2024. </p>
    
        <h3>Sherry Yang</h3>
        <p><strong>Talk Title:</strong> Video Generation as Real-World Simulators </p>
        <p><strong>Abstract:</strong> Generative models have transformed content creation, and the next frontier may be simulating realistic experiences in response to actions by humans and agents. In this talk, I will talk about a line of work that involves learning a real-world simulator to emulate interactions through generative modeling of video content. I will also talk about the applications of a real-world simulator, including training vision-language planners and reinforcement learning policies, which have demonstrated zero-shot real-world transfer. Lastly, I will talk about how to improve generative simulators from real-world feedback. </p>
        <p><strong>Bio:</strong> Sherry is an incoming assistant professor of Computer Science at NYU Courant, a post-doc at Stanford University, and a research scientist at Google DeepMind. Her research aims to develop machine learning models with internet-scale knowledge to make better-than-human decisions. To this end, her work has pioneered representation learning and generative modeling from large vision and language data coupled with algorithms for sequential decision making such as imitation learning, planning, and reinforcement learning. Her research UniSim: Learning Interactive Real-World Simulators has been recognized by the Outstanding Paper award at ICLR. Prior to her current roles, Sherry received her PhD in Computer Science at UC Berkeley and her Bachelor's and Master’s degree in Electrical Engineering and Computer Science at MIT. </p>

        <h3>Yinqing He</h3>
        <p><strong>Talk Title:</strong> Video Generation as Real-World Simulators </p>
        <p><strong>Abstract:</strong> In light of the recent progress of large language models (LLMs), there is a growing interest in integrating LLMs with abilities of processing multiple modalities, especially image and video. This tutorial delves into the breakthrough of AI-generated content, such as images and videos, examining the current status, the milestones achieved, and the challenges that remain. Given the formidable capabilities of LLMs, we pose the question: Can LLMs enhance the generation of images and videos? If they can, in what ways can LLMs assist in the generation of images and videos? To answer this question, we conduct a comprehensive review of related works and summarize the various roles that LLMs can take in the field of image and video generation, including serving as a unified backbone, planner, captioner, conditioner, evaluator, and agent. We hope this tutorial can provide the audience with a clear understanding of the current status of image and video generation, and how LLMs function within it, in order to promote better integration of LLMs with image and video generation paradigms in the future. </p>
        <p><strong>Bio:</strong> Yingqing He is a final-year PhD student at HKUST, under the supervision of Prof. Qifeng Chen. Her research interests are text-to-video generation, controllable generation and multimodal generation. Her featured works include LVDM, VideoCrafter1, Follow-your-pose, Animate-A-Story, and ScaleCrafter. </p>
    </div>
</section>

<section class="wrapper style3 container special" id="speakers">
    <header class="major">
      <h2><strong>Speakers</strong></h2>
    </header>

    <div class="row">

      <div class="4u">
        <a href="https://thoth.inrialpes.fr/~schmid/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/cs.jpg" alt="Cordelia Schmid" />
            <header>
              <h3>
                <strong> Cordelia <br /> Schmid </strong>
              </h3>
            </header>
            <p style="text-align: center;">INRIA</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://people.csail.mit.edu/ganchuang/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/gc.png" alt="Chuang Gan" />
            <header>
              <h3>
                <strong> Chuang <br /> Gan </strong>
              </h3>
            </header>
            <p style="text-align: center;">MIT-IBM AI Lab</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://people.eecs.berkeley.edu/~kanazawa/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/ak.png" alt="Angjoo Kanazawa" />
            <header>
              <h3>
                <strong> Angjoo <br /> Kanazawa </strong>
              </h3>
            </header>
            <p style="text-align: center;">University of California at Berkeley</p>
          </section>
        </a>
      </div>

    </div>

    <div class="row">

      <div class="4u">
        <a href="https://sites.google.com/view/showlab" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/mzs.png" alt="Mike Z. Shou" />
            <header>
              <h3>
                <strong> Mike Z. <br /> Shou </strong>
              </h3>
            </header>
            <p style="text-align: center;">National University of Singapore</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://sites.google.com/view/fromandto" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/hz.jpg" alt="Hao Zhao" />
            <header>
              <h3>
                <strong> Hao <br /> Zhao </strong>
              </h3>
            </header>
            <p style="text-align: center;">Tsinghua University</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://sherryy.github.io/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/sy.jpg" alt="Sherry Yang" />
            <header>
              <h3>
                <strong> Sherry <br /> Yang </strong>
              </h3>
            </header>
            <p style="text-align: center;">Google DeepMind</p>
          </section>
        </a>
      </div>

    </div>

    <div class="row">

      <div class="2u">
        <a href="" target="_blank">
          <section>
            <img class="image-circle" src="images/blank.jpg" alt="" />
            <header>
              <h5>
                <strong> <br /> </strong>
              </h5>
            </header>
            <p style="text-align: center;"></p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://kashyap7x.github.io/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/kc.jpg" alt="Kashyap Chitta" />
            <header>
              <h3>
                <strong> Kashyap <br /> Chitta </strong>
              </h3>
            </header>
            <p style="text-align: center;">University of Tübingen</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://yingqinghe.github.io/" target="_blank">
          <section>
            <img class="image-circle" src="images/speakers/yqh.png" alt="Yingqing He" />
            <header>
              <h3>
                <strong> Yingqing <br /> He </strong>
              </h3>
            </header>
            <p style="text-align: center;">HKUST</p>
          </section>
        </a>
      </div>

      <div class="2u">
        <a href="" target="_blank">
          <section>
            <img class="image-circle" src="images/blank.jpg" alt="" />
            <header>
              <h5>
                <strong> <br /> </strong>
              </h5>
            </header>
            <p style="text-align: center;"></p>
          </section>
        </a>
      </div>

    </div>

</section>

<section class="wrapper style3 container special" id="organizers">
    <header class="major">
      <h2><strong>Organizers</strong></h2>
    </header>
    <p style="text-align: right; font-size: small;">* Equal Contribution</p>

    <div class="row">

      <div class="4u">
        <a href="https://andytang15.github.io/" target="_blank">
          <section>
            <img class="image-circle" src="images/organizers/yst.jpg" alt="Yansong Tang" />
            <header>
              <h5>
                <strong> Yansong <br /> Tang* </strong>
              </h5>
            </header>
            <p style="text-align: center;">Tsinghua University</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://ailingzeng.site/" target="_blank">
          <section>
            <img class="image-circle" src="images/organizers/alz.png" alt="Ailing Zeng" />
            <header>
              <h5>
                <strong> Ailing <br /> Zeng* </strong>
              </h5>
            </header>
            <p style="text-align: center;">Tencent</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://sites.google.com/view/showlab" target="_blank">
          <section>
            <img class="image-circle" src="images/organizers/mzs.png" alt="Mike Z. Shou" />
            <header>
              <h5>
                <strong> Mike Z. <br /> Shou </strong>
              </h5>
            </header>
            <p style="text-align: center;">National University of Singapore</p>
          </section>
        </a>
      </div>

    </div>

    <div class="row">

      <div class="2u">
        <a href="" target="_blank">
          <section>
            <img class="image-circle" src="images/blank.jpg" alt="" />
            <header>
              <h5>
                <strong> <br /> </strong>
              </h5>
            </header>
            <p style="text-align: center;"></p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://www.surrey.ac.uk/people/josef-kittler" target="_blank">
          <section>
            <img class="image-circle" src="images/organizers/jk.png" alt="Josef Kittler" />
            <header>
              <h5>
                <strong> Josef <br /> Kittler </strong>
              </h5>
            </header>
            <p style="text-align: center;">University of Surrey</p>
          </section>
        </a>
      </div>

      <div class="4u">
        <a href="https://eng.ox.ac.uk/people/philip-torr/" target="_blank">
          <section>
            <img class="image-circle" src="images/organizers/pt.jpeg" alt="Philip H.S. Torr" />
            <header>
              <h5>
                <strong> Philip H.S. <br /> Torr </strong>
              </h5>
            </header>
            <p style="text-align: center;">University of Oxford</p>
          </section>
        </a>
      </div>

      <div class="2u">
        <a href="" target="_blank">
          <section>
            <img class="image-circle" src="images/blank.jpg" alt="" />
            <header>
              <h5>
                <strong> <br /> </strong>
              </h5>
            </header>
            <p style="text-align: center;"></p>
          </section>
        </a>
      </div>

    </div>

</section>

</article>

</div>
<footer id="footer">
    <ul class="icons"></ul>
    <ul class="copyright">
        <li>&copy; <a href="https://github.com/Dai-Wenxun">Wenxun Dai</a> 2024. Adapted from <a href="https://ngr-co3d.github.io/"> NGR-CO3D </a> and <a href="https://dynavis.github.io/">DynaVis</a>.</li>
    </ul>
</footer>
</body>
</html>
